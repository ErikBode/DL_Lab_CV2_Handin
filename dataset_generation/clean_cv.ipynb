{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mbode-karl-erik\u001b[0m (\u001b[33merikbodedev\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/erik/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import huggingface_hub\n",
    "\n",
    "\n",
    "if run_local:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # TODO - write your own token here\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    wandb_token = os.getenv(\"wandb_api_key\")\n",
    "    huggingface_hub.login(token=hf_token)\n",
    "\n",
    "    # login into the clients\n",
    "    wandb.login(key=wandb_token)\n",
    "    huggingface_hub.login(token=hf_token)\n",
    "\n",
    "else:\n",
    "    if run_on_kaggle:\n",
    "        # access the secrets\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "        user_secrets = UserSecretsClient()\n",
    "\n",
    "        # fetch the tokens from secrets\n",
    "        wandb_token = user_secrets.get_secret(\"wandb_api_key\")\n",
    "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "        # login into the clients\n",
    "        wandb.login(key=wandb_token)\n",
    "        huggingface_hub.login(token=hf_token)\n",
    "    else:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        # get the token from the userdata\n",
    "        hf_token = userdata.get(\"HF_TOKEN\")\n",
    "        wandb_token = userdata.get(\"wandb_api_key\")\n",
    "\n",
    "        # login into the clients\n",
    "        wandb.login(key=wandb_token)\n",
    "        huggingface_hub.login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the datasets\n",
    "common_voice = load_dataset(\"mozilla-foundation/common_voice_13_0\", \"en\", trust_remote_code=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 1013968\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 16372\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 16372\n",
       "    })\n",
       "    other: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 278333\n",
       "    })\n",
       "    invalidated: Dataset({\n",
       "        features: ['client_id', 'path', 'audio', 'sentence', 'up_votes', 'down_votes', 'age', 'gender', 'accent', 'locale', 'segment', 'variant'],\n",
       "        num_rows: 264713\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_modified = copy.deepcopy(common_voice)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_voice_modified = common_voice_modified.remove_columns([\"client_id\",\"path\", \"audio\",\"up_votes\", \"down_votes\",\"age\", \"gender\", \"accent\", \"locale\", \"segment\", \"variant\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 1013968\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 16372\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 16372\n",
       "    })\n",
       "    other: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 278333\n",
       "    })\n",
       "    invalidated: Dataset({\n",
       "        features: ['sentence'],\n",
       "        num_rows: 264713\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "common_voice_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only keep train, validation and test subsets\n",
    "import datasets\n",
    "temp = datasets.DatasetDict({\"train\": common_voice_modified[\"train\"], \"validation\": common_voice_modified[\"validation\"], \"test\": common_voice_modified[\"test\"]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = temp.rename_column(\"sentence\", \"text\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for empty entries in train and validation and remove them\n",
    "temp = temp.filter(function=lambda x: len(x[\"text\"]) > 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cast to lowercase\n",
    "temp = temp.map(function=lambda x: {\"text\": x[\"text\"].lower()})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 961433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14017\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000\n",
    "for line in temp[\"train\"][\"text\"]:\n",
    "    temp_len = len(line)\n",
    "    if temp_len > max_len:\n",
    "        max_len = temp_len\n",
    "    if temp_len < min_len:\n",
    "        min_len = temp_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(229, 31)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len, min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f348dbadf5924343a658b373f41057b2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/961433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75d56a644ed648a39aa44eecaf9ed039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28c92c7f81174419bb32655ac9705fb3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/14017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp.save_to_disk(\"kaggle/input/train-datasets/cv_mod_30.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/erik/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: write).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data to see min length of the text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# TODO - write your own token here\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_token = os.getenv(\"wandb_api_key\")\n",
    "huggingface_hub.login(token=hf_token)\n",
    "\n",
    "# login into the clients\n",
    "wandb.login(key=wandb_token)\n",
    "huggingface_hub.login(token=hf_token)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 961433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14452\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 14017\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = temp[\"train\"]\n",
    "data_validation = temp[\"validation\"]\n",
    "data_test = temp[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf \"kaggle/working/temp_data_sets\"\n",
    "!mkdir \"kaggle/working/temp_data_sets\"\n",
    "\n",
    "working_dir = \"kaggle/working/temp_data_sets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0666ea2abd0d470c923c4eda8fc1a4c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/961433 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f66dfceb89d94b62ad712c7789dec38f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14452 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6942edacb5624af38d2ce17474376fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/14017 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_length = 16\n",
    "# Map function to apply tokenization and caching - TODO use formatting function to avoid code redudancy\n",
    "train_data = data_train.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",    # Pad to the maximum sequence length\n",
    "        truncation=True,         # Truncate sequences longer than max_length\n",
    "        max_length=max_length,          # Maximum sequence length\n",
    "        return_attention_mask=True,  # Return attention masks\n",
    "        return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_train.cache\"\n",
    ")\n",
    "\n",
    "\n",
    "val_data = data_validation.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_valid.cache\"\n",
    ")\n",
    "\n",
    "test_data = data_test.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_test.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_enc = 1000\n",
    "max_enc = 0\n",
    "for enc_data in [np.array(train_data[\"input_ids\"]), np.array(val_data[\"input_ids\"]), np.array(test_data[\"input_ids\"])]:\n",
    "    # enc data is 2D array\n",
    "    # find the min and max length of the encoded data excluding zeros\n",
    "    for enc in enc_data:\n",
    "        temp_len = len(enc[enc != 0])\n",
    "        if temp_len < min_enc:\n",
    "            min_enc = temp_len\n",
    "        if temp_len > max_enc:\n",
    "            max_enc = temp_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 16)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_enc, max_enc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
