{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_local = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/erik/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n",
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "import wandb\n",
    "import huggingface_hub\n",
    "\n",
    "\n",
    "if run_local:\n",
    "    import os\n",
    "    from dotenv import load_dotenv\n",
    "\n",
    "    load_dotenv()\n",
    "\n",
    "    # TODO - write your own token here\n",
    "    hf_token = os.getenv(\"HF_TOKEN\")\n",
    "    wandb_token = os.getenv(\"wandb_api_key\")\n",
    "    huggingface_hub.login(token=hf_token)\n",
    "\n",
    "    # login into the clients\n",
    "    wandb.login(key=wandb_token)\n",
    "    huggingface_hub.login(token=hf_token)\n",
    "\n",
    "else:\n",
    "    if run_on_kaggle:\n",
    "        # access the secrets\n",
    "        from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "        user_secrets = UserSecretsClient()\n",
    "\n",
    "        # fetch the tokens from secrets\n",
    "        wandb_token = user_secrets.get_secret(\"wandb_api_key\")\n",
    "        hf_token = user_secrets.get_secret(\"HF_TOKEN\")\n",
    "\n",
    "        # login into the clients\n",
    "        wandb.login(key=wandb_token)\n",
    "        huggingface_hub.login(token=hf_token)\n",
    "    else:\n",
    "        from google.colab import userdata\n",
    "\n",
    "        # get the token from the userdata\n",
    "        hf_token = userdata.get(\"HF_TOKEN\")\n",
    "        wandb_token = userdata.get(\"wandb_api_key\")\n",
    "\n",
    "        # login into the clients\n",
    "        wandb.login(key=wandb_token)\n",
    "        huggingface_hub.login(token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Load the datasets\n",
    "voxpopuli = load_dataset(\n",
    "    \"esb/datasets\", \"voxpopuli\", trust_remote_code=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['audio', 'dataset', 'text', 'id'],\n",
       "        num_rows: 182482\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['audio', 'dataset', 'text', 'id'],\n",
       "        num_rows: 1753\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['audio', 'dataset', 'text', 'id'],\n",
       "        num_rows: 1842\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "voxpopuli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxpopuli_modified = copy.deepcopy(voxpopuli)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "voxpopuli_modified = voxpopuli_modified.remove_columns([\"audio\", \"dataset\", \"id\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "277befc260044d10957a082449244f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/182482 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43a08acf2b494221aab01d834680960b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1753 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "420355e9f8004df5a11273804c20b80b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Filter:   0%|          | 0/1842 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# check for empty entries in train and validation and remove them\n",
    "voxpopuli_modified = voxpopuli_modified.filter(function=lambda x: len(x[\"text\"]) > 90)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "frac = len(voxpopuli_modified[\"validation\"])/len(voxpopuli_modified[\"train\"])\n",
    "temp = voxpopuli_modified[\"train\"].train_test_split(test_size=frac)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp[\"validation\"] = voxpopuli_modified[\"validation\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 133647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1245\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 0\n",
    "min_len = 1000\n",
    "for line in temp[\"train\"][\"text\"]:\n",
    "    temp_len = len(line)\n",
    "    if temp_len > max_len:\n",
    "        max_len = temp_len\n",
    "    if temp_len < min_len:\n",
    "        min_len = temp_len\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2905, 91)"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max_len, min_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9843829bf494f55a05346bb74b8ee44",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/133647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c08b57aa1604ee9a1788812d3f12bfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e304a65d5c934f5a9fea4fe5586938bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "temp.save_to_disk(\"kaggle/input/train-datasets/vp_mod_90.hf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /home/erik/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\n",
      "Token is valid (permission: read).\n",
      "Your token has been saved to /home/erik/.cache/huggingface/token\n",
      "Login successful\n"
     ]
    }
   ],
   "source": [
    "# tokenize the data to see min length of the text\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "# TODO - write your own token here\n",
    "hf_token = os.getenv(\"HF_TOKEN\")\n",
    "wandb_token = os.getenv(\"wandb_api_key\")\n",
    "huggingface_hub.login(token=hf_token)\n",
    "\n",
    "# login into the clients\n",
    "wandb.login(key=wandb_token)\n",
    "huggingface_hub.login(token=hf_token)\n",
    "\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"google/gemma-2b\"\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#     load_in_4bit=True,\n",
    "#     bnb_4bit_quant_type=\"nf4\",\n",
    "#     bnb_4bit_compute_dtype=torch.bfloat16\n",
    "# )\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, token=hf_token)\n",
    "tokenizer.padding_side = 'right'\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=bnb_config, device_map={\"\":0}, token=hf_token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 133647\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1245\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1245\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = temp[\"train\"]\n",
    "data_validation = temp[\"validation\"]\n",
    "data_test = temp[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "!rm -rf \"kaggle/working/temp_data_sets\"\n",
    "!mkdir \"kaggle/working/temp_data_sets\"\n",
    "\n",
    "working_dir = \"kaggle/working/temp_data_sets/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c76a8bc0eaa4a7998010347859256f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/133647 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0df78aeb1dc64df2991fe28987949abf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "80e2640260c14e13ac8df637a006c0e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1245 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "max_length = 16\n",
    "# Map function to apply tokenization and caching - TODO use formatting function to avoid code redudancy\n",
    "train_data = data_train.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",    # Pad to the maximum sequence length\n",
    "        truncation=True,         # Truncate sequences longer than max_length\n",
    "        max_length=max_length,          # Maximum sequence length\n",
    "        return_attention_mask=True,  # Return attention masks\n",
    "        return_tensors=\"pt\"      # Return PyTorch tensors\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_train.cache\"\n",
    ")\n",
    "\n",
    "\n",
    "val_data = data_validation.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_valid.cache\"\n",
    ")\n",
    "\n",
    "test_data = data_test.map(\n",
    "    lambda examples: tokenizer(\n",
    "        examples[\"text\"],\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=max_length,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    ),\n",
    "    batched=True,\n",
    "    cache_file_name=working_dir + \"vp_test.cache\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_enc = 1000\n",
    "max_enc = 0\n",
    "for enc_data in [np.array(train_data[\"input_ids\"]), np.array(val_data[\"input_ids\"]), np.array(test_data[\"input_ids\"])]:\n",
    "    # enc data is 2D array\n",
    "    # find the min and max length of the encoded data excluding zeros\n",
    "    for enc in enc_data:\n",
    "        temp_len = len(enc[enc != 0])\n",
    "        if temp_len < min_enc:\n",
    "            min_enc = temp_len\n",
    "        if temp_len > max_enc:\n",
    "            max_enc = temp_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(12, 16)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_enc, max_enc"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Lab_Project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
